import h5py
import pandas as pd
import numpy as np
import cv2
import os
import os.path as osp
import argparse
from tqdm import tqdm
import datetime

# --- å‚æ•°é…ç½® ---
PARSER = argparse.ArgumentParser(description="Convert SEVIR to SimVP H5 (Clone Logic)")
PARSER.add_argument('--raw_root', type=str, default='./data/sevir', help='CATALOG.csv æ‰€åœ¨çš„æ ¹ç›®å½•')
PARSER.add_argument('--output_path', type=str, default='./data/sevir/sevir.h5', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
PARSER.add_argument('--img_size', type=int, default=128, help='ç›®æ ‡åˆ†è¾¨ç‡')
args = PARSER.parse_args()

# --- æ ¸å¿ƒé…ç½® ---
SEQ_LEN = 25
STRIDE = 12


# å·²ç§»é™¤ MIN_VIL_THRESH å’Œ MIN_COVERAGE

def find_file_path_clone(root_dir, filename):
    """
    å®Œå…¨å¤åˆ» generate_sevir.py çš„ _open_files é€»è¾‘
    """
    # 1. ç¡®å®š data_home
    # generate_sevir.py é»˜è®¤ data_home æ˜¯ 'data/sevir/data'
    # å¦‚æœ root_dir ä¸‹é¢æœ‰ä¸ª data æ–‡ä»¶å¤¹ï¼Œæˆ‘ä»¬ä¼˜å…ˆç”¨å®ƒ
    potential_data_home = osp.join(root_dir, 'data')
    if osp.exists(potential_data_home):
        data_home = potential_data_home
    else:
        data_home = root_dir

    # 2. å¤åˆ» if-elif æŸ¥æ‰¾é€»è¾‘
    # é€»è¾‘æ¥æºï¼šgenerate_sevir.py -> SEVIRDataset._open_files

    # å°è¯• A: ç›´æ¥æ‹¼æ¥
    possible_path = osp.join(data_home, filename)
    if osp.exists(possible_path):
        return possible_path

    # å°è¯• B: ç‰¹æ®Šè§„åˆ™åŒ¹é…
    basename = osp.basename(filename)

    # è§„åˆ™ 1: å¦‚æœè·¯å¾„é‡Œæ²¡ vil ä¸” data_home ä¹Ÿæ²¡ vilï¼Œæ‹¼ä¸€ä¸ª vil è¿›å»
    if 'vil' not in filename and 'vil' not in data_home:
        path = osp.join(data_home, 'vil', filename)
        if osp.exists(path): return path

    # è§„åˆ™ 2: æŒ‰å¹´ä»½æš´åŠ›åŒ¹é… (2017, 2018, 2019)
    if '2017' in filename:
        path = osp.join(data_home, 'vil', '2017', basename)
        if osp.exists(path): return path
    elif '2018' in filename:
        path = osp.join(data_home, 'vil', '2018', basename)
        if osp.exists(path): return path
    elif '2019' in filename:
        path = osp.join(data_home, 'vil', '2019', basename)
        if osp.exists(path): return path

    return None


def get_sliding_windows(event_data):
    num_frames = event_data.shape[0]
    samples = []
    for start_idx in range(0, num_frames - SEQ_LEN + 1, STRIDE):
        end_idx = start_idx + SEQ_LEN
        sample = event_data[start_idx:end_idx]
        samples.append(sample)
    return samples


def resize_seq(data, target_size=128):
    resized = []
    for img in data:
        img_r = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_LINEAR)
        resized.append(img_r)
    return np.array(resized)


def process_and_write(split_name, events, raw_root, hf_out):
    print(f"\nğŸš€ Processing {split_name} set... Raw Events: {len(events)}")

    if split_name in hf_out: del hf_out[split_name]
    grp = hf_out.create_group(split_name)

    dset = grp.create_dataset('data',
                              shape=(0, SEQ_LEN, args.img_size, args.img_size),
                              maxshape=(None, SEQ_LEN, args.img_size, args.img_size),
                              dtype='uint8',
                              chunks=(1, SEQ_LEN, args.img_size, args.img_size),
                              compression="lzf")

    grouped = events.groupby('file_name')
    buffer = []
    buffer_limit = 500
    total_samples = 0
    missing_files = set()

    pbar = tqdm(total=len(events))

    for file_name, group in grouped:
        # --- ä½¿ç”¨å¤åˆ»çš„æŸ¥æ‰¾é€»è¾‘ ---
        file_path = find_file_path_clone(raw_root, file_name)

        if file_path is None:
            if file_name not in missing_files:
                missing_files.add(file_name)
            pbar.update(len(group))
            continue

        try:
            with h5py.File(file_path, 'r') as hf_in:
                # å…¼å®¹ key
                raw_dataset = hf_in['vil'] if 'vil' in hf_in else hf_in[list(hf_in.keys())[0]]

                for _, row in group.iterrows():
                    idx = int(row['file_index'])
                    raw_event = raw_dataset[idx]

                    if raw_event.ndim == 3 and raw_event.shape[-1] == 49:
                        raw_event = raw_event.transpose(2, 0, 1)

                    slices = get_sliding_windows(raw_event)

                    for s in slices:
                        # ã€ä¿®æ”¹ã€‘æ— è¿‡æ»¤ï¼Œç›´æ¥ resize å¹¶æ·»åŠ 
                        s_resized = resize_seq(s, args.img_size)
                        buffer.append(s_resized)

                    pbar.update(1)

                    if len(buffer) >= buffer_limit:
                        current_len = dset.shape[0]
                        add_len = len(buffer)
                        dset.resize(current_len + add_len, axis=0)
                        dset[current_len:] = np.array(buffer, dtype='uint8')
                        total_samples += add_len
                        buffer = []

        except Exception as e:
            print(f"Error reading {file_name}: {e}")
            pbar.update(len(group))
            continue

    if len(buffer) > 0:
        current_len = dset.shape[0]
        dset.resize(current_len + len(buffer), axis=0)
        dset[current_len:] = np.array(buffer, dtype='uint8')
        total_samples += len(buffer)

    pbar.close()
    if len(missing_files) > 0:
        print(f"âš ï¸ Warning: {len(missing_files)} files missing.")
        # print(list(missing_files)[:3]) # æ‰“å°å‰3ä¸ªçœ‹çœ‹
    print(f"âœ… {split_name} Done. Valid Samples: {total_samples}")


def main():
    # 1. è¯»å– Catalog
    # ä¼˜å…ˆæ‰¾ raw_root ä¸‹çš„ CATALOGï¼Œæ‰¾ä¸åˆ°å°±æ‰¾ raw_root ä¸Šä¸€å±‚çš„ï¼ˆé˜²æ­¢æŒ‡å‘äº† data/sevir/dataï¼‰
    catalog_path = os.path.join(args.raw_root, 'CATALOG.csv')
    if not os.path.exists(catalog_path):
        catalog_path = os.path.join(os.path.dirname(args.raw_root), 'CATALOG.csv')

    if not os.path.exists(catalog_path):
        raise FileNotFoundError(f"CATALOG.csv not found in or above {args.raw_root}")

    print(f"ğŸ“– Loading Catalog from {catalog_path}...")
    catalog = pd.read_csv(catalog_path, parse_dates=['time_utc'], low_memory=False)

    # åŸºç¡€æ ¡éªŒï¼šåªå– VIL å’Œ å®Œæ•´å›¾
    catalog = catalog[catalog['img_type'] == 'vil']
    catalog = catalog[catalog['pct_missing'] == 0]
    print(f"Filtered Catalog: {len(catalog)} events.")

    # 2. åˆ’åˆ†
    val_start = datetime.datetime(2019, 6, 1)
    test_start = datetime.datetime(2019, 10, 1)

    train_df = catalog[catalog['time_utc'] < val_start]
    val_df = catalog[(catalog['time_utc'] >= val_start) & (catalog['time_utc'] < test_start)]
    test_df = catalog[catalog['time_utc'] >= test_start]

    print(f"Split Sizes -> Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

    # 3. æ‰§è¡Œ
    if os.path.exists(args.output_path):
        os.remove(args.output_path)
    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)

    with h5py.File(args.output_path, 'w') as hf_out:
        process_and_write('train', train_df, args.raw_root, hf_out)
        process_and_write('val', val_df, args.raw_root, hf_out)
        process_and_write('test', test_df, args.raw_root, hf_out)

    print(f"\nğŸ‰ All Done! Saved to {args.output_path}")


if __name__ == '__main__':
    main()