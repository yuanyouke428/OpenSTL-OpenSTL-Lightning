import torch
import torch.nn as nn
import time

# å°è¯•å¯¼å…¥ Mambaï¼Œå¦‚æœæŠ¥é”™è¯´æ˜ç¯å¢ƒæ²¡è£…å¥½
try:
    from mamba_ssm import Mamba

    print("âœ… Mamba-ssm å¯¼å…¥æˆåŠŸï¼")
except ImportError:
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° mamba_ssmï¼Œè¯·æ£€æŸ¥ pip install æ˜¯å¦æˆåŠŸã€‚")
    exit()


class SimpleMambaLayer(nn.Module):
    def __init__(self, dim, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.dim = dim
        # Mamba å®˜æ–¹æ¨¡å—
        self.mamba = Mamba(
            d_model=dim,  # ç‰¹å¾ç»´åº¦
            d_state=d_state,  # å†…éƒ¨çŠ¶æ€ç»´åº¦
            d_conv=d_conv,  # å±€éƒ¨å·ç§¯å®½åº¦
            expand=expand,  # æ‰©å±•å› å­
        )
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        # x çš„å½¢çŠ¶æ˜¯å›¾ç‰‡: [Batch, Channel, Height, Width]
        B, C, H, W = x.shape
        print(f"1. è¾“å…¥å½¢çŠ¶: {x.shape}")

        # --- å…³é”®æ­¥éª¤ï¼šå›¾ç‰‡è½¬åºåˆ— ---
        # å˜æˆ [Batch, H*W, Channel] -> Mamba éœ€è¦ (B, L, D)
        x_flat = x.permute(0, 2, 3, 1).contiguous().view(B, H * W, C)
        print(f"2. å±•å¹³åå½¢çŠ¶ (ç¬¦åˆMambaè¾“å…¥): {x_flat.shape}")

        # å½’ä¸€åŒ–
        x_norm = self.norm(x_flat)

        # Mamba æ¨ç†
        out = self.mamba(x_norm)
        print(f"3. Mambaè¾“å‡ºå½¢çŠ¶: {out.shape}")

        # --- å…³é”®æ­¥éª¤ï¼šåºåˆ—è½¬å›å›¾ç‰‡ ---
        # å˜å› [Batch, Channel, Height, Width]
        out = out.view(B, H, W, C).permute(0, 3, 1, 2).contiguous()
        print(f"4. æ¢å¤å›¾ç‰‡å½¢çŠ¶: {out.shape}")

        return out + x  # æ®‹å·®è¿æ¥


# --- å¼€å§‹æµ‹è¯• ---
def run_demo():
    # æ£€æŸ¥æ˜¯å¦æœ‰ GPUï¼ŒMamba å¿…é¡»åœ¨ CUDA ä¸Šè·‘ï¼ˆé™¤éä½¿ç”¨æœ€æ–°ç‰ˆçš„ CPU å…¼å®¹æ¨¡å¼ï¼Œä½†å¾ˆæ…¢ï¼‰
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ è¿è¡Œè®¾å¤‡: {device}")

    if device == "cpu":
        print("âš ï¸ è­¦å‘Šï¼šMamba åœ¨ CPU ä¸Šè¿è¡Œå¯èƒ½ä¼šæŠ¥é”™æˆ–ææ…¢ï¼Œå»ºè®®ä½¿ç”¨ GPUã€‚")

    # 1. å®šä¹‰æ¨¡å‹ (å‡è®¾ç‰¹å¾é€šé“æ˜¯ 64)
    model = SimpleMambaLayer(dim=64).to(device)

    # 2. é€ ä¸€ä¸ªå‡æ•°æ® (æ¨¡æ‹Ÿ Batch=2, Channel=64, 64x64 çš„é›·è¾¾å›¾)
    x = torch.randn(2, 64, 64, 64).to(device)

    # 3. è¿è¡Œå‰å‘ä¼ æ’­
    start_time = time.time()
    try:
        y = model(x)
        end_time = time.time()
        print(f"\nâœ… æµ‹è¯•é€šè¿‡ï¼å‰å‘ä¼ æ’­è€—æ—¶: {end_time - start_time:.4f} ç§’")
        print(f"æœ€ç»ˆè¾“å‡ºå¼ é‡å½¢çŠ¶: {y.shape}")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        print("ğŸ’¡ æç¤ºï¼šå¦‚æœæ˜¯ CUDA ç›¸å…³é”™è¯¯ï¼Œè¯·æ£€æŸ¥ PyTorch å’Œ CUDA ç‰ˆæœ¬æ˜¯å¦åŒ¹é…ã€‚")


if __name__ == "__main__":
    run_demo()